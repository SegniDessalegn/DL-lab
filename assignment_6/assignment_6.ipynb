{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Afan Oromoo Text Generation Using RNN LSTM**"]},{"cell_type":"markdown","metadata":{},"source":["# Segni Dessalegn"]},{"cell_type":"markdown","metadata":{},"source":["# UGR/8961/12"]},{"cell_type":"markdown","metadata":{},"source":["Afaan Oromoo text generation implemented using Pytorch"]},{"cell_type":"markdown","metadata":{},"source":["# import libraries"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-03T10:46:09.259465Z","iopub.status.busy":"2024-02-03T10:46:09.259160Z","iopub.status.idle":"2024-02-03T10:46:09.278621Z","shell.execute_reply":"2024-02-03T10:46:09.277881Z","shell.execute_reply.started":"2024-02-03T10:46:09.259407Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['assignment_6.ipynb', 'rnn.net', 'data.txt']\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","\n","import os\n","print(os.listdir(\"./\"))\n","\n","# Any results you write to the current directory are saved as output.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2024-02-03T10:46:09.280050Z","iopub.status.busy":"2024-02-03T10:46:09.279828Z","iopub.status.idle":"2024-02-03T10:46:09.283619Z","shell.execute_reply":"2024-02-03T10:46:09.282840Z","shell.execute_reply.started":"2024-02-03T10:46:09.280012Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n"]},{"cell_type":"code","execution_count":6,"metadata":{"_uuid":"f2583f727bb1d09183f67a421d15c7e29abf7939","execution":{"iopub.execute_input":"2024-02-03T10:46:09.293957Z","iopub.status.busy":"2024-02-03T10:46:09.293653Z","iopub.status.idle":"2024-02-03T10:46:10.247706Z","shell.execute_reply":"2024-02-03T10:46:10.246669Z","shell.execute_reply.started":"2024-02-03T10:46:09.293900Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["total 17360\n","drwxrwxr-x 2 segni segni     4096 Feb  3 14:33 .\n","drwxrwxr-x 4 segni segni     4096 Feb  3 14:26 ..\n","-rw-rw-r-- 1 segni segni    39681 Feb  3 14:59 assignment_6.ipynb\n","-rw-rw-r-- 1 segni segni  3866497 Feb  1 15:57 data.txt\n","-rw-rw-r-- 1 segni segni 13857793 Feb  3 14:08 rnn.net\n"]}],"source":["!ls -al ./\n"]},{"cell_type":"markdown","metadata":{},"source":["# Load dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"_uuid":"30f496a18bb2f868f01bc6513fa89c05d72a280e","execution":{"iopub.execute_input":"2024-02-03T10:46:10.249246Z","iopub.status.busy":"2024-02-03T10:46:10.248973Z","iopub.status.idle":"2024-02-03T10:46:10.299095Z","shell.execute_reply":"2024-02-03T10:46:10.298336Z","shell.execute_reply.started":"2024-02-03T10:46:10.249196Z"},"trusted":true},"outputs":[],"source":["text = (open(\"data.txt\").read())\n"]},{"cell_type":"code","execution_count":9,"metadata":{"_uuid":"627e3b9ee06272fb51d3b4b60b863f74138e1c19","execution":{"iopub.execute_input":"2024-02-03T10:46:10.300418Z","iopub.status.busy":"2024-02-03T10:46:10.300183Z","iopub.status.idle":"2024-02-03T10:46:10.307470Z","shell.execute_reply":"2024-02-03T10:46:10.306568Z","shell.execute_reply.started":"2024-02-03T10:46:10.300375Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"Dhaabbanni keenya Waldaan Aksiyoona Faanaa Broodkaastiing Koorporeet bara 1987 yammuu hundaa'u\\nQajee\""]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["text[:100]\n"]},{"cell_type":"code","execution_count":10,"metadata":{"_uuid":"bba6b02280d46faed7ef221a6236459ee20f61ea","execution":{"iopub.execute_input":"2024-02-03T10:46:10.308719Z","iopub.status.busy":"2024-02-03T10:46:10.308515Z","iopub.status.idle":"2024-02-03T10:46:11.226213Z","shell.execute_reply":"2024-02-03T10:46:11.225574Z","shell.execute_reply.started":"2024-02-03T10:46:10.308683Z"},"trusted":true},"outputs":[],"source":["chars = tuple(set(text))\n","int2char = dict(enumerate(chars))\n","char2int = {ch: ii for ii, ch in int2char.items()}\n","encoded = np.array([char2int[ch] for ch in text])\n"]},{"cell_type":"markdown","metadata":{},"source":["# One hot encoder"]},{"cell_type":"code","execution_count":11,"metadata":{"_uuid":"ee67ec61e590a16c2cae9b586159b16d4da07f6b","execution":{"iopub.execute_input":"2024-02-03T10:46:11.228079Z","iopub.status.busy":"2024-02-03T10:46:11.227811Z","iopub.status.idle":"2024-02-03T10:46:11.232869Z","shell.execute_reply":"2024-02-03T10:46:11.232116Z","shell.execute_reply.started":"2024-02-03T10:46:11.228026Z"},"trusted":true},"outputs":[],"source":["def one_hot_encode(arr, n_labels):\n","\n","    # Initialize the the encoded array\n","    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n","\n","    # Fill the appropriate elements with ones\n","    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n","\n","    # Finally reshape it to get back to the original array\n","    one_hot = one_hot.reshape((*arr.shape, n_labels))\n","\n","    return one_hot\n"]},{"cell_type":"markdown","metadata":{},"source":["# Get batches of data"]},{"cell_type":"code","execution_count":12,"metadata":{"_uuid":"8030d36f98270b4308946f9baa088757c866dc55","execution":{"iopub.execute_input":"2024-02-03T10:46:11.234386Z","iopub.status.busy":"2024-02-03T10:46:11.234071Z","iopub.status.idle":"2024-02-03T10:46:11.243122Z","shell.execute_reply":"2024-02-03T10:46:11.242394Z","shell.execute_reply.started":"2024-02-03T10:46:11.234275Z"},"trusted":true},"outputs":[],"source":["def get_batches(arr, n_seqs, n_steps):\n","    '''Create a generator that returns mini-batches of size\n","       n_seqs x n_steps from arr.\n","    '''\n","\n","    batch_size = n_seqs * n_steps\n","    n_batches = len(arr)//batch_size\n","\n","    # Keep only enough characters to make full batches\n","    arr = arr[:n_batches * batch_size]\n","    # Reshape into n_seqs rows\n","    arr = arr.reshape((n_seqs, -1))\n","\n","    for n in range(0, arr.shape[1], n_steps):\n","        # The features\n","        x = arr[:, n:n+n_steps]\n","        # The targets, shifted by one\n","        y = np.zeros_like(x)\n","        try:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n","        except IndexError:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n","        yield x, y\n"]},{"cell_type":"markdown","metadata":{},"source":["# Implement RNN class"]},{"cell_type":"code","execution_count":13,"metadata":{"_uuid":"24a33ecd09fefca10c507aa162ad07d186b3647e","execution":{"iopub.execute_input":"2024-02-03T10:46:11.244532Z","iopub.status.busy":"2024-02-03T10:46:11.244310Z","iopub.status.idle":"2024-02-03T10:46:11.260669Z","shell.execute_reply":"2024-02-03T10:46:11.259932Z","shell.execute_reply.started":"2024-02-03T10:46:11.244492Z"},"trusted":true},"outputs":[],"source":["class CharRNN(nn.Module):\n","    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n","                               drop_prob=0.6, lr=0.001):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.n_layers = n_layers\n","        self.n_hidden = n_hidden\n","        self.lr = lr\n","\n","        self.chars = tokens\n","        self.int2char = dict(enumerate(self.chars))\n","        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n","\n","        self.dropout = nn.Dropout(drop_prob)\n","        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,\n","                            dropout=drop_prob, batch_first=True)\n","        self.fc = nn.Linear(n_hidden, len(self.chars))\n","\n","        self.init_weights()\n","\n","    def forward(self, x, hc):\n","        ''' Forward pass through the network '''\n","\n","        x, (h, c) = self.lstm(x, hc)\n","        x = self.dropout(x)\n","\n","        # Stack up LSTM outputs\n","        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n","\n","        x = self.fc(x)\n","\n","        return x, (h, c)\n","\n","    def predict(self, char, h=None, cuda=False, top_k=None):\n","        ''' Given a character, predict the next character.\n","\n","            Returns the predicted character and the hidden state.\n","        '''\n","        if cuda:\n","            self.cuda()\n","        else:\n","            self.cpu()\n","\n","        if h is None:\n","            h = self.init_hidden(1)\n","\n","        x = np.array([[self.char2int[char]]])\n","        x = one_hot_encode(x, len(self.chars))\n","        inputs = Variable(torch.from_numpy(x), volatile=True)\n","        if cuda:\n","            inputs = inputs.cuda()\n","\n","        h = tuple([Variable(each.data, volatile=True) for each in h])\n","        out, h = self.forward(inputs, h)\n","\n","        p = F.softmax(out).data\n","        if cuda:\n","            p = p.cpu()\n","\n","        if top_k is None:\n","            top_ch = np.arange(len(self.chars))\n","        else:\n","            p, top_ch = p.topk(top_k)\n","            top_ch = top_ch.numpy().squeeze()\n","\n","        p = p.numpy().squeeze()\n","        char = np.random.choice(top_ch, p=p/p.sum())\n","\n","        return self.int2char[char], h\n","\n","    def init_weights(self):\n","        ''' Initialize weights for fully connected layer '''\n","        initrange = 0.1\n","\n","        # Set bias tensor to all zeros\n","        self.fc.bias.data.fill_(0)\n","        # FC weights as random uniform\n","        self.fc.weight.data.uniform_(-1, 1)\n","\n","    def init_hidden(self, n_seqs):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n","                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))\n"]},{"cell_type":"markdown","metadata":{},"source":["# A function to train the model\n","\n","Arguments\n","- net: CharRNN network\n","- data: text data to train the network\n","- epochs: Number of epochs to train\n","- n_seqs: Number of mini-sequences per mini-batch, aka batch size\n","- n_steps: Number of character steps per mini-batch\n","- lr: learning rate\n","- clip: gradient clipping\n","- val_frac: Fraction of data to hold out for validation\n","- cuda: Train with CUDA on a GPU\n","- print_every: Number of steps for printing training and validation loss"]},{"cell_type":"code","execution_count":15,"metadata":{"_uuid":"442555fc96a5286d163cfc4e75cbda5ecdb59199","execution":{"iopub.execute_input":"2024-02-03T10:46:11.261923Z","iopub.status.busy":"2024-02-03T10:46:11.261699Z","iopub.status.idle":"2024-02-03T10:46:11.276963Z","shell.execute_reply":"2024-02-03T10:46:11.276147Z","shell.execute_reply.started":"2024-02-03T10:46:11.261883Z"},"trusted":true},"outputs":[],"source":["def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n","    net.train()\n","    opt = torch.optim.Adam(net.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # create training and validation data\n","    val_idx = int(len(data)*(1-val_frac))\n","    data, val_data = data[:val_idx], data[val_idx:]\n","\n","    if cuda:\n","        net.cuda()\n","\n","    counter = 0\n","    n_chars = len(net.chars)\n","    for e in range(epochs):\n","        h = net.init_hidden(n_seqs)\n","        for x, y in get_batches(data, n_seqs, n_steps):\n","            counter += 1\n","\n","            # One-hot encode our data and make them Torch tensors\n","            x = one_hot_encode(x, n_chars)\n","            x, y = torch.from_numpy(x), torch.from_numpy(y)\n","\n","            inputs, targets = Variable(x), Variable(y)\n","            if cuda:\n","                inputs, targets = inputs.cuda(), targets.cuda()\n","\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            h = tuple([Variable(each.data) for each in h])\n","\n","            net.zero_grad()\n","\n","            output, h = net.forward(inputs, h)\n","            loss = criterion(output, targets.view(n_seqs*n_steps))\n","\n","            loss.backward()\n","\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm(net.parameters(), clip)\n","\n","            opt.step()\n","\n","            if counter % print_every == 0:\n","\n","                # Get validation loss\n","                val_h = net.init_hidden(n_seqs)\n","                val_losses = []\n","                for x, y in get_batches(val_data, n_seqs, n_steps):\n","                    # One-hot encode our data and make them Torch tensors\n","                    x = one_hot_encode(x, n_chars)\n","                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n","\n","                    # Creating new variables for the hidden state, otherwise\n","                    # we'd backprop through the entire training history\n","                    val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n","\n","                    inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n","                    if cuda:\n","                        inputs, targets = inputs.cuda(), targets.cuda()\n","\n","                    output, val_h = net.forward(inputs, val_h)\n","                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n","\n","                    val_losses.append(val_loss.data[0])\n","\n","                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                      \"Step: {}...\".format(counter),\n","                      \"Loss: {:.4f}...\".format(loss.data[0]),\n","                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n"]},{"cell_type":"code","execution_count":16,"metadata":{"_uuid":"e6a0180219ccf5efc1d4a612f1a31df04a09a36e","execution":{"iopub.execute_input":"2024-02-03T10:46:11.278583Z","iopub.status.busy":"2024-02-03T10:46:11.278261Z","iopub.status.idle":"2024-02-03T10:46:11.286734Z","shell.execute_reply":"2024-02-03T10:46:11.285899Z","shell.execute_reply.started":"2024-02-03T10:46:11.278527Z"},"trusted":true},"outputs":[],"source":["if 'net' in locals():\n","    del net\n"]},{"cell_type":"code","execution_count":17,"metadata":{"_uuid":"fd0815d898725cd9e5e7b227bf8017ff7e802117","execution":{"iopub.execute_input":"2024-02-03T10:46:11.288018Z","iopub.status.busy":"2024-02-03T10:46:11.287809Z","iopub.status.idle":"2024-02-03T10:46:11.336625Z","shell.execute_reply":"2024-02-03T10:46:11.335898Z","shell.execute_reply.started":"2024-02-03T10:46:11.287973Z"},"trusted":true},"outputs":[],"source":["net = CharRNN(chars, n_hidden=512, n_layers=2)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Start the training process"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n","/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n","/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:87: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1/10... Step: 10... Loss: 3.3574... Val Loss: 3.3108\n","Epoch: 1/10... Step: 20... Loss: 3.2052... Val Loss: 3.1832\n","Epoch: 1/10... Step: 30... Loss: 3.0958... Val Loss: 3.0593\n","Epoch: 1/10... Step: 40... Loss: 2.9181... Val Loss: 2.9155\n","Epoch: 1/10... Step: 50... Loss: 2.7964... Val Loss: 2.7698\n","Epoch: 1/10... Step: 60... Loss: 2.6432... Val Loss: 2.6392\n","Epoch: 1/10... Step: 70... Loss: 2.5528... Val Loss: 2.5388\n","Epoch: 1/10... Step: 80... Loss: 2.5145... Val Loss: 2.4706\n","Epoch: 1/10... Step: 90... Loss: 2.4507... Val Loss: 2.4283\n","Epoch: 1/10... Step: 100... Loss: 2.3650... Val Loss: 2.3909\n","Epoch: 1/10... Step: 110... Loss: 2.4027... Val Loss: 2.3572\n","Epoch: 1/10... Step: 120... Loss: 2.3391... Val Loss: 2.3339\n","Epoch: 1/10... Step: 130... Loss: 2.3419... Val Loss: 2.3089\n","Epoch: 1/10... Step: 140... Loss: 2.2672... Val Loss: 2.2827\n","Epoch: 1/10... Step: 150... Loss: 2.2627... Val Loss: 2.2641\n","Epoch: 1/10... Step: 160... Loss: 2.2437... Val Loss: 2.2469\n","Epoch: 1/10... Step: 170... Loss: 2.2405... Val Loss: 2.2283\n","Epoch: 1/10... Step: 180... Loss: 2.1775... Val Loss: 2.2124\n","Epoch: 1/10... Step: 190... Loss: 2.1840... Val Loss: 2.1906\n","Epoch: 1/10... Step: 200... Loss: 2.1487... Val Loss: 2.1780\n","Epoch: 1/10... Step: 210... Loss: 2.1318... Val Loss: 2.1622\n","Epoch: 1/10... Step: 220... Loss: 2.1328... Val Loss: 2.1451\n","Epoch: 1/10... Step: 230... Loss: 2.1250... Val Loss: 2.1328\n","Epoch: 1/10... Step: 240... Loss: 2.1557... Val Loss: 2.1172\n","Epoch: 1/10... Step: 250... Loss: 2.0907... Val Loss: 2.1044\n","Epoch: 1/10... Step: 260... Loss: 2.1017... Val Loss: 2.0933\n","Epoch: 1/10... Step: 270... Loss: 2.1085... Val Loss: 2.0839\n","Epoch: 2/10... Step: 280... Loss: 2.1081... Val Loss: 2.0738\n","Epoch: 2/10... Step: 290... Loss: 2.0456... Val Loss: 2.0594\n","Epoch: 2/10... Step: 300... Loss: 2.0585... Val Loss: 2.0498\n","Epoch: 2/10... Step: 310... Loss: 2.0092... Val Loss: 2.0394\n","Epoch: 2/10... Step: 320... Loss: 2.0140... Val Loss: 2.0313\n","Epoch: 2/10... Step: 330... Loss: 2.0020... Val Loss: 2.0216\n","Epoch: 2/10... Step: 340... Loss: 1.9852... Val Loss: 2.0141\n","Epoch: 2/10... Step: 350... Loss: 2.0352... Val Loss: 2.0017\n","Epoch: 2/10... Step: 360... Loss: 2.0043... Val Loss: 1.9925\n","Epoch: 2/10... Step: 370... Loss: 1.9703... Val Loss: 1.9831\n","Epoch: 2/10... Step: 380... Loss: 2.0263... Val Loss: 1.9774\n","Epoch: 2/10... Step: 390... Loss: 1.9510... Val Loss: 1.9732\n","Epoch: 2/10... Step: 400... Loss: 1.9787... Val Loss: 1.9614\n","Epoch: 2/10... Step: 410... Loss: 1.9377... Val Loss: 1.9528\n","Epoch: 2/10... Step: 420... Loss: 1.9384... Val Loss: 1.9459\n","Epoch: 2/10... Step: 430... Loss: 1.9308... Val Loss: 1.9382\n","Epoch: 2/10... Step: 440... Loss: 1.9368... Val Loss: 1.9307\n","Epoch: 2/10... Step: 450... Loss: 1.9057... Val Loss: 1.9267\n","Epoch: 2/10... Step: 460... Loss: 1.9078... Val Loss: 1.9164\n","Epoch: 2/10... Step: 470... Loss: 1.8656... Val Loss: 1.9133\n","Epoch: 2/10... Step: 480... Loss: 1.8486... Val Loss: 1.9036\n","Epoch: 2/10... Step: 490... Loss: 1.8603... Val Loss: 1.8964\n","Epoch: 2/10... Step: 500... Loss: 1.8705... Val Loss: 1.8886\n","Epoch: 2/10... Step: 510... Loss: 1.9081... Val Loss: 1.8828\n","Epoch: 2/10... Step: 520... Loss: 1.8493... Val Loss: 1.8769\n","Epoch: 2/10... Step: 530... Loss: 1.8580... Val Loss: 1.8713\n","Epoch: 2/10... Step: 540... Loss: 1.8763... Val Loss: 1.8642\n","Epoch: 3/10... Step: 550... Loss: 1.8868... Val Loss: 1.8607\n","Epoch: 3/10... Step: 560... Loss: 1.8289... Val Loss: 1.8542\n","Epoch: 3/10... Step: 570... Loss: 1.8519... Val Loss: 1.8494\n","Epoch: 3/10... Step: 580... Loss: 1.8134... Val Loss: 1.8489\n","Epoch: 3/10... Step: 590... Loss: 1.8056... Val Loss: 1.8432\n","Epoch: 3/10... Step: 600... Loss: 1.8101... Val Loss: 1.8358\n","Epoch: 3/10... Step: 610... Loss: 1.7904... Val Loss: 1.8305\n","Epoch: 3/10... Step: 620... Loss: 1.8398... Val Loss: 1.8258\n","Epoch: 3/10... Step: 630... Loss: 1.8329... Val Loss: 1.8209\n","Epoch: 3/10... Step: 640... Loss: 1.7714... Val Loss: 1.8144\n","Epoch: 3/10... Step: 650... Loss: 1.8515... Val Loss: 1.8126\n","Epoch: 3/10... Step: 660... Loss: 1.7831... Val Loss: 1.8042\n","Epoch: 3/10... Step: 670... Loss: 1.7877... Val Loss: 1.8033\n","Epoch: 3/10... Step: 680... Loss: 1.7695... Val Loss: 1.8078\n","Epoch: 3/10... Step: 690... Loss: 1.7855... Val Loss: 1.7997\n","Epoch: 3/10... Step: 700... Loss: 1.7714... Val Loss: 1.7935\n","Epoch: 3/10... Step: 710... Loss: 1.7725... Val Loss: 1.7879\n","Epoch: 3/10... Step: 720... Loss: 1.7739... Val Loss: 1.7813\n","Epoch: 3/10... Step: 730... Loss: 1.7542... Val Loss: 1.7770\n","Epoch: 3/10... Step: 740... Loss: 1.7178... Val Loss: 1.7745\n","Epoch: 3/10... Step: 750... Loss: 1.6931... Val Loss: 1.7752\n","Epoch: 3/10... Step: 760... Loss: 1.7348... Val Loss: 1.7698\n","Epoch: 3/10... Step: 770... Loss: 1.7345... Val Loss: 1.7686\n","Epoch: 3/10... Step: 780... Loss: 1.7869... Val Loss: 1.7703\n","Epoch: 3/10... Step: 790... Loss: 1.7294... Val Loss: 1.7587\n","Epoch: 3/10... Step: 800... Loss: 1.7362... Val Loss: 1.7561\n","Epoch: 3/10... Step: 810... Loss: 1.7673... Val Loss: 1.7539\n","Epoch: 4/10... Step: 820... Loss: 1.7572... Val Loss: 1.7525\n","Epoch: 4/10... Step: 830... Loss: 1.7206... Val Loss: 1.7460\n","Epoch: 4/10... Step: 840... Loss: 1.7288... Val Loss: 1.7460\n","Epoch: 4/10... Step: 850... Loss: 1.6941... Val Loss: 1.7435\n","Epoch: 4/10... Step: 860... Loss: 1.6897... Val Loss: 1.7366\n","Epoch: 4/10... Step: 870... Loss: 1.6891... Val Loss: 1.7358\n","Epoch: 4/10... Step: 880... Loss: 1.6913... Val Loss: 1.7342\n","Epoch: 4/10... Step: 890... Loss: 1.7319... Val Loss: 1.7323\n","Epoch: 4/10... Step: 900... Loss: 1.7294... Val Loss: 1.7284\n","Epoch: 4/10... Step: 910... Loss: 1.6818... Val Loss: 1.7256\n","Epoch: 4/10... Step: 920... Loss: 1.7462... Val Loss: 1.7211\n","Epoch: 4/10... Step: 930... Loss: 1.6746... Val Loss: 1.7183\n","Epoch: 4/10... Step: 940... Loss: 1.6995... Val Loss: 1.7131\n","Epoch: 4/10... Step: 950... Loss: 1.6818... Val Loss: 1.7137\n","Epoch: 4/10... Step: 960... Loss: 1.6951... Val Loss: 1.7180\n","Epoch: 4/10... Step: 970... Loss: 1.6801... Val Loss: 1.7077\n","Epoch: 4/10... Step: 980... Loss: 1.6881... Val Loss: 1.7050\n","Epoch: 4/10... Step: 990... Loss: 1.6924... Val Loss: 1.7041\n","Epoch: 4/10... Step: 1000... Loss: 1.6783... Val Loss: 1.7003\n","Epoch: 4/10... Step: 1010... Loss: 1.6297... Val Loss: 1.6967\n","Epoch: 4/10... Step: 1020... Loss: 1.6222... Val Loss: 1.6936\n","Epoch: 4/10... Step: 1030... Loss: 1.6614... Val Loss: 1.6917\n","Epoch: 4/10... Step: 1040... Loss: 1.6630... Val Loss: 1.6968\n","Epoch: 4/10... Step: 1050... Loss: 1.7054... Val Loss: 1.6906\n","Epoch: 4/10... Step: 1060... Loss: 1.6548... Val Loss: 1.6909\n","Epoch: 4/10... Step: 1070... Loss: 1.6669... Val Loss: 1.6824\n","Epoch: 4/10... Step: 1080... Loss: 1.6955... Val Loss: 1.6849\n","Epoch: 5/10... Step: 1090... Loss: 1.6792... Val Loss: 1.6819\n","Epoch: 5/10... Step: 1100... Loss: 1.6436... Val Loss: 1.6774\n","Epoch: 5/10... Step: 1110... Loss: 1.6553... Val Loss: 1.6842\n","Epoch: 5/10... Step: 1120... Loss: 1.6322... Val Loss: 1.6726\n","Epoch: 5/10... Step: 1130... Loss: 1.6216... Val Loss: 1.6729\n","Epoch: 5/10... Step: 1140... Loss: 1.6355... Val Loss: 1.6701\n","Epoch: 5/10... Step: 1150... Loss: 1.6158... Val Loss: 1.6698\n","Epoch: 5/10... Step: 1160... Loss: 1.6719... Val Loss: 1.6678\n","Epoch: 5/10... Step: 1170... Loss: 1.6566... Val Loss: 1.6693\n","Epoch: 5/10... Step: 1180... Loss: 1.6203... Val Loss: 1.6682\n","Epoch: 5/10... Step: 1190... Loss: 1.6800... Val Loss: 1.6637\n","Epoch: 5/10... Step: 1200... Loss: 1.6075... Val Loss: 1.6596\n","Epoch: 5/10... Step: 1210... Loss: 1.6266... Val Loss: 1.6581\n","Epoch: 5/10... Step: 1220... Loss: 1.6180... Val Loss: 1.6545\n","Epoch: 5/10... Step: 1230... Loss: 1.6237... Val Loss: 1.6548\n","Epoch: 5/10... Step: 1240... Loss: 1.6085... Val Loss: 1.6517\n","Epoch: 5/10... Step: 1250... Loss: 1.6214... Val Loss: 1.6527\n","Epoch: 5/10... Step: 1260... Loss: 1.6329... Val Loss: 1.6488\n","Epoch: 5/10... Step: 1270... Loss: 1.6201... Val Loss: 1.6477\n","Epoch: 5/10... Step: 1280... Loss: 1.5659... Val Loss: 1.6471\n","Epoch: 5/10... Step: 1290... Loss: 1.5632... Val Loss: 1.6494\n","Epoch: 5/10... Step: 1300... Loss: 1.5969... Val Loss: 1.6440\n","Epoch: 5/10... Step: 1310... Loss: 1.6109... Val Loss: 1.6415\n","Epoch: 5/10... Step: 1320... Loss: 1.6486... Val Loss: 1.6391\n","Epoch: 5/10... Step: 1330... Loss: 1.6010... Val Loss: 1.6392\n","Epoch: 5/10... Step: 1340... Loss: 1.5994... Val Loss: 1.6381\n","Epoch: 5/10... Step: 1350... Loss: 1.6598... Val Loss: 1.6353\n","Epoch: 6/10... Step: 1360... Loss: 1.6337... Val Loss: 1.6355\n","Epoch: 6/10... Step: 1370... Loss: 1.5948... Val Loss: 1.6352\n","Epoch: 6/10... Step: 1380... Loss: 1.6124... Val Loss: 1.6406\n","Epoch: 6/10... Step: 1390... Loss: 1.5767... Val Loss: 1.6285\n","Epoch: 6/10... Step: 1400... Loss: 1.5674... Val Loss: 1.6268\n","Epoch: 6/10... Step: 1410... Loss: 1.5885... Val Loss: 1.6265\n","Epoch: 6/10... Step: 1420... Loss: 1.5651... Val Loss: 1.6269\n","Epoch: 6/10... Step: 1430... Loss: 1.6205... Val Loss: 1.6210\n","Epoch: 6/10... Step: 1440... Loss: 1.6024... Val Loss: 1.6218\n","Epoch: 6/10... Step: 1450... Loss: 1.5704... Val Loss: 1.6192\n","Epoch: 6/10... Step: 1460... Loss: 1.6355... Val Loss: 1.6200\n","Epoch: 6/10... Step: 1470... Loss: 1.5561... Val Loss: 1.6146\n","Epoch: 6/10... Step: 1480... Loss: 1.5802... Val Loss: 1.6139\n","Epoch: 6/10... Step: 1490... Loss: 1.5749... Val Loss: 1.6183\n","Epoch: 6/10... Step: 1500... Loss: 1.5860... Val Loss: 1.6137\n","Epoch: 6/10... Step: 1510... Loss: 1.5737... Val Loss: 1.6153\n","Epoch: 6/10... Step: 1520... Loss: 1.5809... Val Loss: 1.6099\n","Epoch: 6/10... Step: 1530... Loss: 1.5933... Val Loss: 1.6103\n","Epoch: 6/10... Step: 1540... Loss: 1.5805... Val Loss: 1.6074\n","Epoch: 6/10... Step: 1550... Loss: 1.5166... Val Loss: 1.6063\n","Epoch: 6/10... Step: 1560... Loss: 1.5158... Val Loss: 1.6042\n","Epoch: 6/10... Step: 1570... Loss: 1.5633... Val Loss: 1.6057\n","Epoch: 6/10... Step: 1580... Loss: 1.5749... Val Loss: 1.6039\n","Epoch: 6/10... Step: 1590... Loss: 1.6153... Val Loss: 1.6110\n","Epoch: 6/10... Step: 1600... Loss: 1.5590... Val Loss: 1.6059\n","Epoch: 6/10... Step: 1610... Loss: 1.5660... Val Loss: 1.6027\n","Epoch: 6/10... Step: 1620... Loss: 1.6094... Val Loss: 1.6071\n","Epoch: 7/10... Step: 1630... Loss: 1.5912... Val Loss: 1.5974\n","Epoch: 7/10... Step: 1640... Loss: 1.5512... Val Loss: 1.5977\n","Epoch: 7/10... Step: 1650... Loss: 1.5688... Val Loss: 1.5984\n","Epoch: 7/10... Step: 1660... Loss: 1.5376... Val Loss: 1.5940\n","Epoch: 7/10... Step: 1670... Loss: 1.5275... Val Loss: 1.5943\n","Epoch: 7/10... Step: 1680... Loss: 1.5465... Val Loss: 1.5950\n","Epoch: 7/10... Step: 1690... Loss: 1.5333... Val Loss: 1.5973\n","Epoch: 7/10... Step: 1700... Loss: 1.5729... Val Loss: 1.5918\n","Epoch: 7/10... Step: 1710... Loss: 1.5632... Val Loss: 1.5910\n","Epoch: 7/10... Step: 1720... Loss: 1.5328... Val Loss: 1.5902\n","Epoch: 7/10... Step: 1730... Loss: 1.5910... Val Loss: 1.5868\n","Epoch: 7/10... Step: 1740... Loss: 1.5238... Val Loss: 1.5867\n","Epoch: 7/10... Step: 1750... Loss: 1.5397... Val Loss: 1.5862\n","Epoch: 7/10... Step: 1760... Loss: 1.5469... Val Loss: 1.5842\n","Epoch: 7/10... Step: 1770... Loss: 1.5516... Val Loss: 1.5881\n","Epoch: 7/10... Step: 1780... Loss: 1.5347... Val Loss: 1.5860\n","Epoch: 7/10... Step: 1790... Loss: 1.5534... Val Loss: 1.5835\n","Epoch: 7/10... Step: 1800... Loss: 1.5654... Val Loss: 1.5803\n","Epoch: 7/10... Step: 1810... Loss: 1.5499... Val Loss: 1.5812\n","Epoch: 7/10... Step: 1820... Loss: 1.5041... Val Loss: 1.5823\n","Epoch: 7/10... Step: 1830... Loss: 1.4941... Val Loss: 1.5771\n","Epoch: 7/10... Step: 1840... Loss: 1.5334... Val Loss: 1.5794\n","Epoch: 7/10... Step: 1850... Loss: 1.5437... Val Loss: 1.5758\n","Epoch: 7/10... Step: 1860... Loss: 1.5783... Val Loss: 1.5765\n","Epoch: 7/10... Step: 1870... Loss: 1.5291... Val Loss: 1.5738\n","Epoch: 7/10... Step: 1880... Loss: 1.5302... Val Loss: 1.5712\n","Epoch: 7/10... Step: 1890... Loss: 1.5829... Val Loss: 1.5758\n","Epoch: 8/10... Step: 1900... Loss: 1.5598... Val Loss: 1.5705\n","Epoch: 8/10... Step: 1910... Loss: 1.5163... Val Loss: 1.5704\n","Epoch: 8/10... Step: 1920... Loss: 1.5363... Val Loss: 1.5696\n","Epoch: 8/10... Step: 1930... Loss: 1.5118... Val Loss: 1.5676\n","Epoch: 8/10... Step: 1940... Loss: 1.5009... Val Loss: 1.5657\n","Epoch: 8/10... Step: 1950... Loss: 1.5191... Val Loss: 1.5675\n","Epoch: 8/10... Step: 1960... Loss: 1.5044... Val Loss: 1.5711\n","Epoch: 8/10... Step: 1970... Loss: 1.5516... Val Loss: 1.5653\n","Epoch: 8/10... Step: 1980... Loss: 1.5383... Val Loss: 1.5647\n","Epoch: 8/10... Step: 1990... Loss: 1.5134... Val Loss: 1.5665\n","Epoch: 8/10... Step: 2000... Loss: 1.5727... Val Loss: 1.5645\n","Epoch: 8/10... Step: 2010... Loss: 1.5030... Val Loss: 1.5693\n","Epoch: 8/10... Step: 2020... Loss: 1.5232... Val Loss: 1.5636\n","Epoch: 8/10... Step: 2030... Loss: 1.5134... Val Loss: 1.5671\n","Epoch: 8/10... Step: 2040... Loss: 1.5252... Val Loss: 1.5617\n","Epoch: 8/10... Step: 2050... Loss: 1.5154... Val Loss: 1.5609\n","Epoch: 8/10... Step: 2060... Loss: 1.5213... Val Loss: 1.5600\n","Epoch: 8/10... Step: 2070... Loss: 1.5335... Val Loss: 1.5607\n","Epoch: 8/10... Step: 2080... Loss: 1.5188... Val Loss: 1.5558\n","Epoch: 8/10... Step: 2090... Loss: 1.4667... Val Loss: 1.5594\n","Epoch: 8/10... Step: 2100... Loss: 1.4687... Val Loss: 1.5550\n","Epoch: 8/10... Step: 2110... Loss: 1.5026... Val Loss: 1.5552\n","Epoch: 8/10... Step: 2120... Loss: 1.5095... Val Loss: 1.5544\n","Epoch: 8/10... Step: 2130... Loss: 1.5414... Val Loss: 1.5516\n","Epoch: 8/10... Step: 2140... Loss: 1.5041... Val Loss: 1.5519\n","Epoch: 8/10... Step: 2150... Loss: 1.5043... Val Loss: 1.5518\n","Epoch: 8/10... Step: 2160... Loss: 1.5576... Val Loss: 1.5511\n","Epoch: 9/10... Step: 2170... Loss: 1.5244... Val Loss: 1.5514\n","Epoch: 9/10... Step: 2180... Loss: 1.4963... Val Loss: 1.5527\n","Epoch: 9/10... Step: 2190... Loss: 1.5106... Val Loss: 1.5582\n","Epoch: 9/10... Step: 2200... Loss: 1.4934... Val Loss: 1.5481\n","Epoch: 9/10... Step: 2210... Loss: 1.4776... Val Loss: 1.5476\n","Epoch: 9/10... Step: 2220... Loss: 1.4904... Val Loss: 1.5458\n","Epoch: 9/10... Step: 2230... Loss: 1.4829... Val Loss: 1.5476\n","Epoch: 9/10... Step: 2240... Loss: 1.5294... Val Loss: 1.5463\n","Epoch: 9/10... Step: 2250... Loss: 1.5167... Val Loss: 1.5417\n","Epoch: 9/10... Step: 2260... Loss: 1.4882... Val Loss: 1.5461\n","Epoch: 9/10... Step: 2270... Loss: 1.5440... Val Loss: 1.5476\n","Epoch: 9/10... Step: 2280... Loss: 1.4815... Val Loss: 1.5470\n","Epoch: 9/10... Step: 2290... Loss: 1.4851... Val Loss: 1.5437\n","Epoch: 9/10... Step: 2300... Loss: 1.4939... Val Loss: 1.5417\n","Epoch: 9/10... Step: 2310... Loss: 1.4999... Val Loss: 1.5471\n","Epoch: 9/10... Step: 2320... Loss: 1.4919... Val Loss: 1.5434\n","Epoch: 9/10... Step: 2330... Loss: 1.5013... Val Loss: 1.5422\n","Epoch: 9/10... Step: 2340... Loss: 1.5215... Val Loss: 1.5448\n","Epoch: 9/10... Step: 2350... Loss: 1.5055... Val Loss: 1.5375\n","Epoch: 9/10... Step: 2360... Loss: 1.4441... Val Loss: 1.5377\n","Epoch: 9/10... Step: 2370... Loss: 1.4407... Val Loss: 1.5389\n","Epoch: 9/10... Step: 2380... Loss: 1.4807... Val Loss: 1.5375\n","Epoch: 9/10... Step: 2390... Loss: 1.4894... Val Loss: 1.5379\n","Epoch: 9/10... Step: 2400... Loss: 1.5280... Val Loss: 1.5355\n","Epoch: 9/10... Step: 2410... Loss: 1.4828... Val Loss: 1.5347\n","Epoch: 9/10... Step: 2420... Loss: 1.4894... Val Loss: 1.5333\n","Epoch: 9/10... Step: 2430... Loss: 1.5433... Val Loss: 1.5347\n","Epoch: 10/10... Step: 2440... Loss: 1.5121... Val Loss: 1.5336\n","Epoch: 10/10... Step: 2450... Loss: 1.4773... Val Loss: 1.5324\n","Epoch: 10/10... Step: 2460... Loss: 1.4848... Val Loss: 1.5372\n","Epoch: 10/10... Step: 2470... Loss: 1.4705... Val Loss: 1.5341\n","Epoch: 10/10... Step: 2480... Loss: 1.4562... Val Loss: 1.5310\n","Epoch: 10/10... Step: 2490... Loss: 1.4750... Val Loss: 1.5311\n","Epoch: 10/10... Step: 2500... Loss: 1.4596... Val Loss: 1.5344\n","Epoch: 10/10... Step: 2510... Loss: 1.5025... Val Loss: 1.5337\n","Epoch: 10/10... Step: 2520... Loss: 1.4958... Val Loss: 1.5282\n","Epoch: 10/10... Step: 2530... Loss: 1.4659... Val Loss: 1.5298\n","Epoch: 10/10... Step: 2540... Loss: 1.5206... Val Loss: 1.5262\n","Epoch: 10/10... Step: 2550... Loss: 1.4556... Val Loss: 1.5254\n","Epoch: 10/10... Step: 2560... Loss: 1.4753... Val Loss: 1.5330\n","Epoch: 10/10... Step: 2570... Loss: 1.4682... Val Loss: 1.5250\n","Epoch: 10/10... Step: 2580... Loss: 1.4715... Val Loss: 1.5256\n","Epoch: 10/10... Step: 2590... Loss: 1.4835... Val Loss: 1.5268\n","Epoch: 10/10... Step: 2600... Loss: 1.4792... Val Loss: 1.5249\n","Epoch: 10/10... Step: 2610... Loss: 1.4931... Val Loss: 1.5272\n","Epoch: 10/10... Step: 2620... Loss: 1.4836... Val Loss: 1.5220\n","Epoch: 10/10... Step: 2630... Loss: 1.4210... Val Loss: 1.5210\n","Epoch: 10/10... Step: 2640... Loss: 1.4126... Val Loss: 1.5221\n","Epoch: 10/10... Step: 2650... Loss: 1.4691... Val Loss: 1.5210\n","Epoch: 10/10... Step: 2660... Loss: 1.4763... Val Loss: 1.5203\n","Epoch: 10/10... Step: 2670... Loss: 1.5208... Val Loss: 1.5198\n","Epoch: 10/10... Step: 2680... Loss: 1.4588... Val Loss: 1.5189\n","Epoch: 10/10... Step: 2690... Loss: 1.4642... Val Loss: 1.5167\n","Epoch: 10/10... Step: 2700... Loss: 1.5297... Val Loss: 1.5202\n"]}],"source":["n_seqs, n_steps = 128, 100\n","train(net, encoded, epochs=10, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Save the model parameters"]},{"cell_type":"code","execution_count":21,"metadata":{"_uuid":"0d4888af06907b302d71423cad5c70f4f82092dc","execution":{"iopub.execute_input":"2024-02-03T10:52:35.818700Z","iopub.status.busy":"2024-02-03T10:52:35.818428Z","iopub.status.idle":"2024-02-03T10:52:35.844621Z","shell.execute_reply":"2024-02-03T10:52:35.843837Z","shell.execute_reply.started":"2024-02-03T10:52:35.818660Z"},"trusted":true},"outputs":[],"source":["checkpoint = {'n_hidden': net.n_hidden,\n","              'n_layers': net.n_layers,\n","              'state_dict': net.state_dict(),\n","              'tokens': net.chars}\n","with open('rnn.net', 'wb') as f:\n","    torch.save(checkpoint, f)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Load the saved model"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def load_model(model_path, model_class=CharRNN):\n","    model_dict = torch.load(model_path, map_location=torch.device('cpu'))\n","\n","    state_dict = model_dict['state_dict']\n","\n","    model = model_class(tokens=model_dict['tokens'],\n","                        n_hidden=model_dict['n_hidden'],\n","                        n_layers=model_dict['n_layers'])\n","\n","    model.load_state_dict(state_dict)\n","\n","    return model\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["net = load_model(\"rnn.net\")\n"]},{"cell_type":"code","execution_count":22,"metadata":{"_uuid":"9e597f6954da0459554634a77799367b67e6cc2f","execution":{"iopub.execute_input":"2024-02-03T10:52:01.176780Z","iopub.status.busy":"2024-02-03T10:52:01.176471Z","iopub.status.idle":"2024-02-03T10:52:01.183635Z","shell.execute_reply":"2024-02-03T10:52:01.182927Z","shell.execute_reply.started":"2024-02-03T10:52:01.176728Z"},"trusted":true},"outputs":[],"source":["def sample(net, size, prime='Ani', top_k=None, cuda=False):\n","\n","    if cuda:\n","        net.cuda()\n","    else:\n","        net.cpu()\n","\n","    net.eval()\n","\n","    # First off, run through the prime characters\n","    chars = [ch for ch in prime]\n","    h = net.init_hidden(1)\n","    for ch in prime:\n","        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n","\n","    chars.append(char)\n","\n","\n","    # Now pass in the previous character and get a new one\n","    for ii in range(size):\n","        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n","        chars.append(char)\n","\n","    return ''.join(chars)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Lets see how it works"]},{"cell_type":"code","execution_count":23,"metadata":{"_uuid":"abff2458a13bc7476e7052173ff25426bd763ad7","execution":{"iopub.execute_input":"2024-02-03T10:53:29.419337Z","iopub.status.busy":"2024-02-03T10:53:29.419045Z","iopub.status.idle":"2024-02-03T10:53:30.079549Z","shell.execute_reply":"2024-02-03T10:53:30.078609Z","shell.execute_reply.started":"2024-02-03T10:53:29.419272Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_27684/354380418.py:49: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  inputs = Variable(torch.from_numpy(x), volatile=True)\n","/tmp/ipykernel_27684/354380418.py:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  h = tuple([Variable(each.data, volatile=True) for each in h])\n","/tmp/ipykernel_27684/354380418.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  p = F.softmax(out).data\n"]},{"name":"stdout","output_type":"stream","text":["Haala akka harka keenyaa beeksiseera.\n","Ati hin jedhama nannicha bulchina isinuu dhiiste\n","Ani abbaa isaa argatee jecha taee aramaa akkamaa keessa jirtin balleessiisan kaan duula haadha isaa akka isaa irratti gabbisan\n","Kanaan akkumi keenyaa bara beekamuutii isaani arraba jiru kaayya hundi kanumaa kan hidhaman kee irraa bulchu kijibaa\n","akka isiin goota koomoo inni baatee biyya keenyaa keessaa dhalachuf harreen kana hiidaa keessa jedhe  Akka keetin garee kijibaa kuni biyya balleessa kana hoo argame miti ammoo\n"]}],"source":["print(sample(net, 500, prime='Haala', top_k=5, cuda=False))\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4386971,"sourceId":7532144,"sourceType":"datasetVersion"}],"dockerImageVersionId":11307,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

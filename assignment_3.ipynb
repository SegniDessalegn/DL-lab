{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer():\n",
    "  def __init__(self, features, neurons):\n",
    "    self.weight = 0.01 * torch.rand(neurons, features)\n",
    "    self.biases = torch.zeros(1, neurons)\n",
    "  def forward(self, inputs):\n",
    "    self.output = torch.matmul(inputs, self.weight.T) + self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationRelu:\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    output = torch.max(inputs, torch.tensor(0.0))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_SoftMax:\n",
    "  def forward(self, inputs):\n",
    "    #calculate powers\n",
    "    power_x = torch.exp(inputs)\n",
    "    #get shape\n",
    "    shape_x = inputs.shape\n",
    "    sum_x = torch.sum(power_x, axis = 1, keepdims = True)\n",
    "    #divide\n",
    "    result = power_x / sum_x\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid:\n",
    "  def forward(self, inputs):\n",
    "    shape_x = inputs.shape\n",
    "    ones = torch.full(shape_x, 1)\n",
    "    sum_x = ones + inputs\n",
    "    res = ones / sum_x\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = ActivationRelu()\n",
    "sigmoid = Activation_Sigmoid()\n",
    "softmax = Activation_SoftMax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 18])\n",
      "torch.Size([10, 18])\n",
      "torch.Size([10, 3])\n",
      "tensor([[0.3159, 0.3673, 0.3168],\n",
      "        [0.3223, 0.3547, 0.3230],\n",
      "        [0.3164, 0.3649, 0.3188],\n",
      "        [0.3171, 0.3639, 0.3190],\n",
      "        [0.3201, 0.3593, 0.3206],\n",
      "        [0.3233, 0.3525, 0.3242],\n",
      "        [0.3275, 0.3439, 0.3286],\n",
      "        [0.3217, 0.3540, 0.3243],\n",
      "        [0.3186, 0.3621, 0.3193],\n",
      "        [0.3192, 0.3599, 0.3208]])\n",
      "torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "manual_seed = 42\n",
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "# Number of features\n",
    "features = 4\n",
    "# Neurons in input layers\n",
    "neuron_1, neuron_2, neuron_3 = 18, 18, 18\n",
    "# Output class\n",
    "output_class = 3\n",
    "# Number of samples\n",
    "samples = 10\n",
    "\n",
    "\n",
    "lower_bound = 0\n",
    "upper_bound = 10000\n",
    "input = (upper_bound - lower_bound) * torch.rand(samples, features) + lower_bound\n",
    "\n",
    "layer_1 = DenseLayer(features, neuron_1)\n",
    "layer_1.forward(input)\n",
    "output_1 = relu.forward(layer_1.output)\n",
    "print(output_1.shape)\n",
    "\n",
    "layer_2 = DenseLayer(output_1.shape[1], neuron_2)\n",
    "layer_2.forward(output_1)\n",
    "output_2 = relu.forward(layer_2.output)\n",
    "print(output_2.shape)\n",
    "\n",
    "\n",
    "output_layer = DenseLayer(output_2.shape[1], output_class)\n",
    "output_layer.forward(output_2)\n",
    "print(output_layer.output.shape)\n",
    "final_output_1 = softmax.forward(output_layer.output)\n",
    "print(final_output_1)\n",
    "print(final_output_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 18])\n",
      "torch.Size([10, 18])\n",
      "tensor([[0.3318, 0.3359, 0.3323],\n",
      "        [0.3318, 0.3359, 0.3323],\n",
      "        [0.3318, 0.3359, 0.3323],\n",
      "        [0.3318, 0.3359, 0.3323],\n",
      "        [0.3318, 0.3359, 0.3323],\n",
      "        [0.3318, 0.3359, 0.3323],\n",
      "        [0.3318, 0.3359, 0.3323],\n",
      "        [0.3318, 0.3359, 0.3323],\n",
      "        [0.3318, 0.3359, 0.3323],\n",
      "        [0.3318, 0.3359, 0.3323]])\n",
      "torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "manual_seed = 42\n",
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "# Number of features\n",
    "features = 4\n",
    "# Neurons in input layers\n",
    "neuron_1, neuron_2, neuron_3 = 18, 18, 18\n",
    "# Output class\n",
    "output_classes = 3\n",
    "# Number of samples\n",
    "samples = 10\n",
    "\n",
    "\n",
    "input = torch.rand(samples, features)\n",
    "\n",
    "layer_1 = DenseLayer(features, neuron_1)\n",
    "layer_1.forward(input)\n",
    "output_1 = sigmoid.forward(layer_1.output)\n",
    "print(output_1.shape)\n",
    "\n",
    "layer_2 = DenseLayer(output_1.shape[1], neuron_2)\n",
    "layer_2.forward(output_1)\n",
    "output_2 = sigmoid.forward(layer_2.output)\n",
    "print(output_2.shape)\n",
    "\n",
    "\n",
    "output_layer = DenseLayer(output_2.shape[1], output_classes)\n",
    "output_layer.forward(output_2)\n",
    "final_output_2 = softmax.forward(output_layer.output)\n",
    "print(final_output_2)\n",
    "print(final_output_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]]),\n",
       " tensor([2, 1, 0, 2, 1, 1, 0, 2, 1, 2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random values for y_true (one-hot encoded)\n",
    "y_true = torch.eye(output_classes)[torch.randint(output_classes, size=(samples,))]\n",
    "\n",
    "# Generate random values for y_true_label (label-encoded)\n",
    "y_true_label = torch.randint(output_classes, size=(samples,))\n",
    "y_true, y_true_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating log loss for label encoding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First model log loss:  tensor(0.6377)\n",
      "Second model log loss:  tensor(0.6368)\n"
     ]
    }
   ],
   "source": [
    "def log_loss(y_true, y_pred):\n",
    "    if y_true.dim() > 1:  # Check if y_true is one-hot encoded\n",
    "        loss = -torch.mean(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "    else:  # Integer-encoded labels\n",
    "        y_true_one_hot = torch.zeros_like(y_pred)\n",
    "        y_true_one_hot.scatter_(1, y_true.view(-1, 1), 1)\n",
    "        loss = -torch.mean(y_true_one_hot * torch.log(y_pred) + (1 - y_true_one_hot) * torch.log(1 - y_pred))\n",
    "\n",
    "    return loss\n",
    "\n",
    "loss_1 = log_loss(y_true, final_output_1)\n",
    "loss_2 = log_loss(y_true, final_output_2)\n",
    "print(\"First model log loss: \" , loss_1)\n",
    "print(\"Second model log loss: \", loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculating log loss for one-hot encoding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First model log loss:  tensor(0.6377)\n",
      "Second model log loss:  tensor(0.6368)\n"
     ]
    }
   ],
   "source": [
    "loss_1_label = log_loss(y_true, final_output_1)\n",
    "loss_2_label = log_loss(y_true, final_output_2)\n",
    "print(\"First model log loss: \" , loss_1_label)\n",
    "print(\"Second model log loss: \", loss_2_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
